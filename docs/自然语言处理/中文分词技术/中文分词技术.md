# 中文分词技术

## 1. 中文分词概述

### 1.1 什么是词？

词的性质--**齐夫定律**：一个单词的词频与它的词频排名成反比。

### 1.2 分词流派

中文分词目前可归纳为

- 规则分词：简单高效，难以处理新词
- 统计分词：能够较好地应对新词发现
- 混合分词（规则+统计）：实践中常采用
## 2. 规则分词

主要基于词典，常见词典：

- 互联网词库(SogouW， 15万个词条)
- 清华大学开放中文词库(THUOCL)、
- HanLP词库(千万级词条)

局限很大，3种方法没有哪种最优。此外，词典的细粒度不一对分词结果的影响很大

### 1.1正向最大匹配法

- 顾名思义，从头到尾扫描分词

[Python-正向最长匹配](Python-%E6%AD%A3%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D%0D/Python-%E6%AD%A3%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D%0D.md)

### 1.2 逆向最大匹配法

- 顾名思义，从后往前扫描分词

[Python-逆向最长匹配](Python-%E9%80%86%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D/Python-%E9%80%86%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D.md)

### 1.3 双向最大匹配法

- 同时执行正向和逆向最长匹配，若两者的词数不同，则返回词数更少的那一个。
- 若两者的词数相同，返回两者中单字更少的那一个。
- 当单字数也相同时，优先返回逆向最长匹配的结果。

[Python - 双向最长匹配](Python%20-%20%E5%8F%8C%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D/Python%20-%20%E5%8F%8C%E5%90%91%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D.md)

### 基于字典的其他应用：

- 停用词过滤
- 繁简转换
- 拼音转换
## 3. 统计分词

步骤：

1）建立统计语言模型（N元）

2）单词划分及结果概率计算

### 2.1 语言模型n-gram

- N越大，词序信息越丰富，但数据稀疏问题越严峻，计算量也越大

### 2.2 HMM（隐性马尔可夫）模型

- 将分词作为字在字串中的序列标注任务来实现

### 2.3 其他统计分词算法

- 采用CNN、LSTM等深度学习网络自动发现一些模式和特征，然后结合CRF、softmax等分类算法进行分词预测。
## 4. 混合分词

最常见的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。在保证词典分词准确率的基础上，对未登录词和歧义词有较好的识别。

## 5. 准确率评估

在P、R、F1基础上进行改进，以单词在文本的区间构建元素，如：

分词结果：[1,2],[3,3],[4,5],[6,7,8][9,9]

标准结果：[1,2],[3,3],[4,4],][5,6],[7,8],[9,9]

分词结果∩标准结果：[1,2],[3,3],[9,9]

- P = 分词结果∩标准结果/分词结果 = 3/5 = 60%
- R = 分词结果∩标准结果/标准结果 =3/6 = 50%
- F1 = 55%
## 6. 中文分词语料库

|语料库|字符数|词语种数|总词频|平均词长|
|---|---|---|---|---|
|《人民日报》语料库 PKU|183万|6万|111万|1.6|
|微软亚洲研究院语料库MSR|405万|9万|237万|1.7|
|台湾中央研究院 AS(繁体)|837万|14万|545万|1.5|
|香港城市大学 CITYU(繁体)|240万|7万|146万|1.7|



一般采用MSR作为分词语料的首选，有以下原因：

- 标注一致性上MSR要优于PKU。
- 切分颗粒度上MSR要优于PKU，MSR的机构名称不予切分，而PKU拆开。
- MSR中姓名作为一个整体，更符合习惯。
- MSR量级是PKU的两倍。

